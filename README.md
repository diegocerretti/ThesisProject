This thesis project explores the power scaling laws in complex datasets, focusing on understanding and modeling the statistical structures that govern such phenomena. The study begins with an empirical analysis of the MNIST dataset, examining the power law trends in its feature-feature covariance spectrum. These initial findings are then generalized to larger, synthetically generated datasets. By constructing both linear and nonlinear feature functions, the research maps latent data to various feature representation spaces. A joint statistical model of these feature maps and synthetic datasets is developed, capturing the broad empirical properties observed in real-world data.

Through extensive simulations and analytical computations, the study demonstrates how the test loss of models behaves under varying dataset sizes and feature representations. The results confirm the applicability of power scaling laws, with the test loss trends closely matching those predicted by existing theoretical frameworks. However, some deviations were noted, primarily due to computational limitations in optimizing hyperparameters and ridge parameters.

The research contributes to a deeper understanding of the interplay between dataset size, model complexity, and performance. It also highlights potential avenues for future research, including the exploration of more complex datasets such as audio and time-series data, and the application of alternative nonlinear mapping functions beyond ReLU.
